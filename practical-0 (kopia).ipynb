{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "dataset = CIFAR10(root='data/', download=True, transform=transform)\n",
    "test_data = CIFAR10(root='data/', train=False, transform=transform)\n",
    "\n",
    "val_size = 5000\n",
    "train_size = len(dataset) - val_size\n",
    "\n",
    "train_data, val_data = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "BATCH_SIZE = 10\n",
    "SHUFFLE = False\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=SHUFFLE)\n",
    "val_loader = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=SHUFFLE)\n",
    "test_loader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=SHUFFLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_batch(dl): \n",
    "    for images,lables in dl: \n",
    "        fig, ax = plt.subplots(figsize = (10,10))\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.imshow(make_grid(images,10).permute(1,2,0))\n",
    "        break\n",
    "    \n",
    "# show a batch\n",
    "show_batch(train_loader)\n",
    "show_batch(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class NetLReLU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NetLReLU, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.af = nn.LeakyReLU(0.1)\n",
    "        self.fc1 = nn.Linear(1600, 120)\n",
    "        self.fc2 = nn.Linear(120, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Implement the forward function in the network\n",
    "        x = self.conv1(x)\n",
    "        x = self.af(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.af(x)\n",
    "        x = x.flatten(1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.af(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetTanH(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NetTanH, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.af = nn.Tanh()\n",
    "        self.fc1 = nn.Linear(1600, 120)\n",
    "        self.fc2 = nn.Linear(120, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Implement the forward function in the network\n",
    "        x = self.conv1(x)\n",
    "        x = self.af(x)\n",
    "        x = self.pool(x)\n",
    "        x = self.conv2(x)\n",
    "        x = x.flatten(1)\n",
    "        x = self.af(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.af(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def train_model(model, criterion, optimizer, train_loader, val_loader, num_epochs = 10, show_plot = True, run_name = \"\"):\n",
    "    min_loss = 10000\n",
    "    # Track loss\n",
    "    training_loss, validation_loss = [], []\n",
    "    # Track accuracy\n",
    "    training_acc, validation_acc = [], []\n",
    "    print(\"Training started!\")\n",
    "    for i in range(num_epochs):\n",
    "        # Track loss\n",
    "        epoch_training_loss, epoch_validation_loss = 0, 0\n",
    "        # track accuracy\n",
    "        train_correct, val_correct = 0, 0\n",
    "        # training\n",
    "        model.train(True)\n",
    "        print(\"Train true\")\n",
    "        for batch_nr, (data, labels) in enumerate(train_loader):\n",
    "            # predict\n",
    "            pred = model(data)\n",
    "            # calculate accuracy\n",
    "            _,preds = torch.max(pred,dim=1)\n",
    "            train_correct += torch.sum(preds==labels).item()\n",
    "            # Clear stored gradient values\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(pred, labels)\n",
    "            # Backpropagate the loss through the network to find the gradients of all parameters\n",
    "            loss.backward()\n",
    "            # Update the parameters along their gradients\n",
    "            optimizer.step()\n",
    "            # Update loss\n",
    "            epoch_training_loss += loss.detach().numpy()\n",
    "            print(\"batch nr:\" , batch_nr, \"len\", len(train_loader))\n",
    "        # validation\n",
    "        print(\"starting validation epoch\", i)\n",
    "        model.eval()\n",
    "        for batch_nr, (data, labels) in enumerate(val_loader):\n",
    "            \n",
    "            # predict\n",
    "            pred = model(data)\n",
    "            print(\"predicts val\", pred)\n",
    "            # calculate accuracy\n",
    "            _,preds = torch.max(pred,dim=1)\n",
    "            val_correct += torch.sum(preds==labels).item()\n",
    "             \n",
    "            # calculate loss\n",
    "            loss = criterion(pred, labels)\n",
    "            \n",
    "            # check if loss is smaller than before, if so safe model\n",
    "            if loss < min_loss:\n",
    "                torch.save(model, 'best_model.pt')\n",
    "                min_loss = loss\n",
    "            \n",
    "            # Update loss\n",
    "            epoch_validation_loss += loss.detach().numpy()  \n",
    "        # Save loss for plot\n",
    "        training_loss.append(epoch_training_loss/train_size)\n",
    "        validation_loss.append(epoch_validation_loss/val_size)\n",
    "        # Save accuracy for plot\n",
    "        training_acc.append(train_correct/train_size)\n",
    "        validation_acc.append(val_correct/val_size)\n",
    "        # Print loss every 5 epochs\n",
    "        if i % 5 == 0:\n",
    "            print(f'Epoch {i}, training loss: {training_loss[-1]}, validation loss: {validation_loss[-1]}')\n",
    "            print(f'Train accuracy = {train_correct/train_size}')\n",
    "            print(f'Validation accuracy = {val_correct/val_size}')\n",
    "        writer.add_scalar('training loss ' + run_name,\n",
    "                        training_loss[-1],\n",
    "                        i + 1)\n",
    "        writer.add_scalar('training acc ' + run_name,\n",
    "                        train_correct/train_size,\n",
    "                        i + 1)\n",
    "        writer.add_scalar('validation acc ' + run_name,\n",
    "                        val_correct/val_size,\n",
    "                        i + 1)\n",
    "        #writer.add_scalar('train and validation acc ' + run_name,\n",
    "        #               {train acc:training_acc[-1], val acc:validation_acc[-1]},\n",
    "        #               i + 1)\n",
    "        \n",
    "    if show_plot:\n",
    "        # Plot training and validation loss\n",
    "        epoch = np.arange(len(training_loss))\n",
    "        plt.figure(figsize=(8,4), dpi=100)\n",
    "        plt.plot(epoch, training_loss, 'r', label='Training loss',)\n",
    "        plt.plot(epoch, validation_loss, 'b', label='Validation loss')\n",
    "        plt.legend()\n",
    "        plt.xlabel('Epoch'), plt.ylabel('Loss')\n",
    "        plt.show()\n",
    "        \n",
    "        # Plot training and validation accuracy\n",
    "        plt.figure(figsize=(8,4), dpi=100)\n",
    "        plt.plot(epoch, training_acc, 'r', label='Training accuracy',)\n",
    "        plt.plot(epoch, validation_acc, 'b', label='Validation accuracy')\n",
    "        plt.legend()\n",
    "        plt.xlabel('Epoch'), plt.ylabel('Accuracy')\n",
    "        plt.show()\n",
    "        \n",
    "    idx = np.argmin(validation_loss)\n",
    "    print(f'lowest loss for validation set: {np.min(validation_loss)}, with an accuracy of {validation_acc[idx]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "def test_model(model, test_loader, run_name = \"\"):\n",
    "    y_pred, y_true = [], []\n",
    "    test_acc = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_nr, (data, labels) in enumerate(test_loader):\n",
    "            pred = model(data)\n",
    "            _,preds = torch.max(pred,dim=1)\n",
    "            test_acc += torch.sum(preds==labels).item()\n",
    "            y_pred.extend(preds.numpy())\n",
    "            y_true.extend(labels.numpy())\n",
    "\n",
    "    test_acc /= len(test_loader.dataset) / 100\n",
    "\n",
    "    print(f\"Test accuracy is {np.round(test_acc)}%.\") \n",
    "    # Confusion matrix\n",
    "    cf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    cm_display = ConfusionMatrixDisplay(confusion_matrix = cf_matrix)\n",
    "    cm_display.plot(colorbar=False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Hyperparams. Set these to reasonable values\n",
    "LEARNING_RATE = 0.0001\n",
    "epochs = 30\n",
    "\n",
    "# Load our network\n",
    "model = NetLReLU()\n",
    "\n",
    "# Define our loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define our optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Train the model\n",
    "train_model(model, criterion, optimizer, train_loader, val_loader, epochs, run_name = \"LReLU Adam\")\n",
    "model = torch.load('best_model.pt')\n",
    "test_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Hyperparams. Set these to reasonable values\n",
    "LEARNING_RATE = 0.001\n",
    "epochs = 30\n",
    "\n",
    "# Load our network\n",
    "model = NetLReLU()\n",
    "\n",
    "# Define our loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define our optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Train the model\n",
    "train_model(model, criterion, optimizer, train_loader, val_loader, epochs, run_name = \"LReLU SGD\")\n",
    "model = torch.load('best_model.pt')\n",
    "test_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Hyperparams. Set these to reasonable values\n",
    "LEARNING_RATE = 0.0001\n",
    "epochs = 30 \n",
    "\n",
    "# Load our network\n",
    "model = NetTanH()\n",
    "\n",
    "# Define our loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define our optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Train the model\n",
    "train_model(model, criterion, optimizer, train_loader, val_loader, epochs, run_name = \"tanh Adam\")\n",
    "model = torch.load('best_model.pt')\n",
    "test_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "def matplotlib_imshow(img, one_channel=False):\n",
    "    if one_channel:\n",
    "        img = img.mean(dim=0)\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    if one_channel:\n",
    "        plt.imshow(npimg, cmap=\"Greys\")\n",
    "    else:\n",
    "        plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "writer = SummaryWriter('runs/CIFAR10')\n",
    "\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "img_grid = torchvision.utils.make_grid(images)\n",
    "\n",
    "matplotlib_imshow(img_grid)\n",
    "\n",
    "writer.add_image('ten_CIFAR10_images', img_grid)\n",
    "\n",
    "writer.add_graph(model, images)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torchvision.models import AlexNet\n",
    "\n",
    "LEARNING_RATE = 0.0001\n",
    "epochs = 30\n",
    "\n",
    "model = AlexNet(num_classes = 10)\n",
    "\n",
    "# Define our loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define our optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Train the model\n",
    "train_model(model, criterion, optimizer, train_loader, val_loader, epochs, run_name = \"AlexNet\")\n",
    "model = torch.load('best_model.pt')\n",
    "test_model(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nnlm2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
